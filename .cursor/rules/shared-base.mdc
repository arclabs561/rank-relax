---
title: "Shared Base Rules for rank-* Repositories"
id: shared-base
description: "Common rules that apply to all rank-* repositories"
priority: 100
alwaysApply: true
---

# Shared Cursor Rules for rank-* Repositories

These rules apply to all rank-* repositories in this workspace:
- rank-fusion
- rank-refine
- rank-relax
- rank-eval

## General Principles

- Prefer simplicity over complexity
- Use real data, not synthetic examples
- Statistical rigor matching pre-AI quality (games/tenzi)
- Code-driven visualizations and documentation
- Comprehensive error handling and validation

## Visualization Standards

- All visualizations must use real data from actual code execution
- Statistical depth: distribution fitting (gamma, beta, normal), hypothesis testing
- Large sample sizes: 1000+ for statistical significance
- Code-driven: Generated by Python scripts (PEP 723 inline dependencies)
- Reproducible: Fixed random seeds, documented data sources

## Error Handling

- All scripts must have robust error handling
- Validate data quality before processing
- Clear error messages with context
- Graceful degradation when possible

## Documentation

- Comprehensive docstrings explaining data sources
- Statistical methods documented
- Output files described
- Quality standards noted

## Path Management

- Support environment variables for configuration
- Try multiple path fallbacks
- Clear error messages when paths not found

## Progress Indicators

- Use tqdm for long-running operations
- Descriptive progress messages
- Progress for all computation loops

## Code Quality

- PEP 723 inline dependencies for Python scripts
- Fixed random seeds for reproducibility
- Algorithms match Rust implementations exactly
- Comprehensive validation

## Anti-Sycophancy

- Challenge assumptions and identify issues
- Present alternative approaches with trade-offs
- Use direct technical language
- Only agree when genuinely superior after analysis

## Statistical Methods

- Distribution fitting: Gamma, Beta, Normal (like tenzi)
- Hypothesis testing: t-tests, ANOVA, effect sizes
- Confidence intervals on all comparisons
- Correlation analysis where appropriate

## Visualization Integration

- All READMEs should include visualization sections
- Relative paths verified and working
- Links to detailed analysis pages
- Data source citations

## Shared Utilities

- Use rank-rank/scripts for shared tools
- Reference rank-rank/hack/viz for visualization patterns
- Keep repo-specific code in individual repos
